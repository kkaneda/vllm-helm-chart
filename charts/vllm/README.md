# vllm

![Version: 0.1.0](https://img.shields.io/badge/Version-0.1.0-informational?style=flat-square) ![AppVersion: 0.6.3](https://img.shields.io/badge/AppVersion-0.6.3-informational?style=flat-square)

vLLM OpenAI-Compatible Server

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| args | list | `["--load-format auto"]` | Full list of arguments can be found here: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server  |
| envVars | list | `[]` | Environment variables, full list can be found here: https://docs.vllm.ai/en/latest/serving/env_vars.html |
| hfToken | string | `""` | If added, a secret is created and mounted to the vLLM pod to pull models from HuggingFace |
| image | object | `{"name":"vllm/vllm-openai","tag":""}` | Docker images hosted here: https://hub.docker.com/r/vllm/vllm-openai/tags  |
| model.contextLength | string | `"8192"` | Maximum context length for the model |
| model.name | string | `"Llama-3.2-1B"` | Name of the model to pull |
| model.organization | string | `"meta-llama"` | Hugging Face organization hosting the model |
| model.temperature | string | `"0.95"` | Model temperature |
| service | object | `{"name":"vllm-llama"}` | Values for the Kubernetes service |
| service.name | string | `"vllm-llama"` | From vLLM docs: please do not name the service as vllm, otherwise environment variables set by Kubernetes might conflict with vLLMâ€™s environment variables, because Kubernetes sets environment variables for each service with the capitalized service name as the prefix |
| volumeSize | string | `"10"` | Values for the PVC |

----------------------------------------------
Autogenerated from chart metadata using [helm-docs v1.13.1](https://github.com/norwoodj/helm-docs/releases/v1.13.1)
